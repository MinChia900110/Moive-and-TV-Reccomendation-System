{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinChia900110/Moive-and-TV-Reccomendation-System/blob/main/Netflix_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Overview\n",
        "\n",
        "This project focuses on building a recommendation engine for movies, similar to the logic used by streaming services like Netflix. Our goal is to analyze user viewing patterns and movie metadata to predict what a user would like to watch next.\n",
        "\n",
        "We are exploring multiple approaches, ranging from simple statistical methods to advanced machine learning techniques:\n",
        "\n",
        "  1. Exploratory Data Analysis (EDA): Understanding the \"Long Tail\" of movie ratings.\n",
        "  2. Content-Based Filtering: Recommending movies based on genre, cast, and plot similarities.\n",
        "  3. Collaborative Filtering: Using Matrix Factorization (SVD) and Nearest Neighbors (KNN) to find similar users.\n",
        "  4. Hybrid Approach: Combining both methods to solve the \"Cold Start\" problem.\n"
      ],
      "metadata": {
        "id": "Ackp6WLuPV_I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bb99403"
      },
      "source": [
        "### Tech Stack\n",
        "\n",
        "This notebook utilizes the following technologies and libraries:\n",
        "\n",
        "*   **Python**: The primary programming language.\n",
        "*   **Google Colab**: The cloud-based environment for running the notebook.\n",
        "*   **Libraries**:\n",
        "    *   `pandas`: For data manipulation and analysis.\n",
        "    *   `numpy`: For numerical operations.\n",
        "    *   `scikit-learn`: For machine learning algorithms.\n",
        "    *   `seaborn`: For statistical data visualization.\n",
        "    *   `matplotlib`: For creating static, animated, and interactive visualizations.\n",
        "    *   `surprise`: A Python scikit for recommender systems.\n",
        "    *   `nmslib`: For efficient similarity search (non-metric space library).\n",
        "\n",
        "### How to Run\n",
        "\n",
        "To successfully execute this notebook, please ensure the necessary datasets are accessible in your Colab environment. You have two primary options:\n",
        "\n",
        "1.  **Upload Datasets**: Upload the required CSV files (`movies.csv`, `ratings.csv`, `links.csv`, `tags.csv`) directly to your Colab session. You can do this by clicking the 'Files' icon on the left sidebar, then selecting 'Upload to session storage'. Please note that files uploaded this way are temporary and will be deleted once the runtime is recycled.\n",
        "\n",
        "2.  **Mount Google Drive**: If your datasets are stored in Google Drive, you can mount your Drive to access them persistently. Use the following code snippet in a code cell to mount your Drive:\n",
        "\n",
        "    ```python\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ```\n",
        "\n",
        "    After mounting, navigate to the correct folder within `/content/drive/My Drive/` to access your files.\n",
        "\n",
        "**Important**: Once your data is accessible, ensure that the file paths used in the notebook (e.g., `pd.read_csv('path/to/your/movies.csv')`) correctly point to the location of your uploaded or mounted datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e69bf5e6"
      },
      "source": [
        "## Phase 1: Data Cleaning & EDA (Visualizing the Long Tail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14f3826e"
      },
      "source": [
        "This phase focuses on thoroughly understanding the structure and characteristics of our movie dataset. Key objectives include:\n",
        "\n",
        "1.  **Understanding Dataset Structure**: Examine the columns, data types, and overall organization of the dataset.\n",
        "2.  **Handling Missing Values**: Identify and appropriately address any missing data to ensure data quality and prevent issues in subsequent analysis.\n",
        "3.  **Exploratory Data Analysis (EDA)**: Conduct a comprehensive EDA to uncover initial insights, patterns, and anomalies within the movie data.\n",
        "4.  **Visualizing the 'Long Tail'**: Specifically, we will visualize the 'long tail' phenomenon, which typically describes a distribution where a small number of items (e.g., popular movies) account for a large share of consumption/ratings, while a large number of items (e.g., niche movies) account for a small share.\n",
        "\n",
        "This initial exploration will lay the groundwork for effective feature engineering and model development."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9ed48ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c8cdca-8e79-4a18-b522-bf63fe29527a"
      },
      "source": [
        "# IMPORTANT: After seeing a ModuleNotFoundError, especially after installing new packages,\n",
        "# please restart your Colab runtime (Runtime > Restart runtime) and then re-run this cell.\n",
        "# This ensures all newly installed modules are correctly loaded\n",
        "!pip install pandas scikit-learn seaborn matplotlib\n",
        "!pip install numpy==1.26.4\n",
        "!pip install scikit-surprise\n",
        "!pip install nmslib\n",
        "#Dataframe\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#Visualisation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#Machine Learning\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "#Recommender Systems and Similarity Search\n",
        "from surprise import Dataset, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "import nmslib\n",
        "\n",
        "#Had issues with different version being incompatible, run th following if you are having issue with compatabiliy\n",
        "#print(f\"Pandas version: {pd.__version__}\")\n",
        "#print(f\"NumPy version: {np.__version__}\")\n",
        "#print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "#print(f\"Surprise version: {surprise.__version__}\")\n",
        "# NMSLIB does not always expose a standard __version__ attribute easily\n",
        "#print(\"NMSLIB imported successfully.\")\n",
        "\n",
        "#Configuration for better visualization\n",
        "%matplotlib inline\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "pd.set_option('display.max_columns', 15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting scikit-surprise\n",
            "  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.16.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2555152 sha256=b2899325eb9242b34d0ca34ef8c1b626539c6a953edb5ba1bf9e496e22b3205d\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n",
            "Collecting nmslib\n",
            "  Downloading nmslib-2.1.2.tar.gz (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.2/197.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2.3 (from nmslib)\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from nmslib) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from nmslib) (1.26.4)\n",
            "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.2-cp312-cp312-linux_x86_64.whl size=14283477 sha256=bf0a18ea59092b4327ccc9d23d9c04f18262e3bf5fd92d3a51e910bef3c9b2b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/8d/d5/4d78a804bbe7acb8d2a39d36853886271c5b0d8b8f7d4f6253\n",
            "Successfully built nmslib\n",
            "Installing collected packages: pybind11, nmslib\n",
            "Successfully installed nmslib-2.1.2 pybind11-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f3cc79e"
      },
      "source": [
        "#Loding the dataset. Download if necessary\n",
        "#Path to the data set\n",
        "movies_path = '/content/movies.csv'\n",
        "ratings_path = '/content/ratings.csv'\n",
        "movies_path.head()\n",
        "ratings_path.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1epFQahSIZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77bf5313"
      },
      "source": [
        "## Phase 2: Building a Content-Based Recommender (TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1198f01"
      },
      "source": [
        "This phase will focus on developing a content-based recommender system, primarily utilizing the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization technique.\n",
        "\n",
        "Key objectives include:\n",
        "\n",
        "1.  **Feature Extraction**: Extract relevant features from movie metadata (e.g., genres, plot keywords, cast, director) to create a comprehensive movie profile.\n",
        "2.  **TF-IDF Vectorization**: Apply TF-IDF to transform the textual movie features into a numerical vector representation.\n",
        "3.  **Similarity Calculation**: Compute the similarity between movies based on their TF-IDF vectors, typically using cosine similarity.\n",
        "4.  **Recommendation Generation**: Develop a mechanism to generate movie recommendations for a user based on movies they have previously liked and the calculated similarities.\n",
        "\n",
        "This phase will demonstrate how to recommend movies based purely on their intrinsic characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65e863c5"
      },
      "source": [
        "**Reasoning**:\n",
        "Finally, I will add an empty code cell as a placeholder for the actual implementation of the content-based recommender system, completing all instructions for this subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "122dbdb7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82396f4c"
      },
      "source": [
        "**Reasoning**:\n",
        "Finally, I will add an empty code cell as a placeholder for the actual implementation of the content-based recommender system, completing all instructions for this subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "540cf75e"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb8c1f7b"
      },
      "source": [
        "## Phase 3: Building a Collaborative Filtering Model (SVD/KNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbdf68d"
      },
      "source": [
        "This phase will focus on implementing and evaluating collaborative filtering models, such as Singular Value Decomposition (SVD) or K-Nearest Neighbors (KNN) based algorithms. Key objectives include:\n",
        "\n",
        "1.  **Data Preparation**: Format the user-item interaction data (ratings) into a suitable structure for collaborative filtering algorithms.\n",
        "2.  **Model Training**: Train collaborative filtering models (e.g., SVD from `surprise` library, or a KNN-based approach).\n",
        "3.  **Recommendation Generation**: Generate recommendations for users based on the trained collaborative filtering model.\n",
        "4.  **Understanding Latent Factors**: For models like SVD, explore the latent factors that represent user preferences and movie characteristics.\n",
        "\n",
        "This phase will leverage the power of user-item interactions to provide recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f101be07"
      },
      "source": [
        "**Reasoning**:\n",
        "The third instruction is to add an empty code cell as a placeholder for the collaborative filtering implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e0b1790"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a26d327"
      },
      "source": [
        "**Reasoning**:\n",
        "The third instruction is to add an empty code cell as a placeholder for the collaborative filtering implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1220adaa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c63730d4"
      },
      "source": [
        "## Phase 4: Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Add a markdown heading for 'Phase 4: Evaluation (RMSE & Top-N Accuracy)' followed by a markdown cell explaining its objectives. Create a code cell placeholder for future implementation of model evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4712e2fc"
      },
      "source": [
        "This final phase will focus on rigorously evaluating the performance of the developed recommender systems (Content-Based and Collaborative Filtering).\n",
        "\n",
        "Key objectives include:\n",
        "\n",
        "1.  **Define Evaluation Metrics**: Utilize appropriate metrics such as Root Mean Squared Error (RMSE) for rating prediction tasks and Top-N Accuracy (e.g., Precision@N, Recall@N, F1@N, NDCG) for ranking tasks.\n",
        "2.  **Cross-Validation Strategy**: Implement a robust cross-validation strategy (e.g., train-test split, k-fold cross-validation) to ensure the generalizability of the models.\n",
        "3.  **Comparative Analysis**: Compare the performance of the Content-Based and Collaborative Filtering models, identifying their strengths and weaknesses.\n",
        "4.  **Analyze Top-N Recommendations**: Evaluate the quality of the top-N recommendations generated by each model.\n",
        "\n",
        "This evaluation will provide insights into which model performs best under different criteria and guide potential future improvements or hybrid approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e730ca9"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I will add an empty code cell as a placeholder for the actual implementation of model evaluation, completing the third instruction for this subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81e186b3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0602d17e"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b313a39"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}